{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor creation\n",
    "\n",
    "https://pytorch.org/docs/stable/tensors.html\n",
    "\n",
    "A torch.Tensor is a multi-dimensional matrix containing elements of a single data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Scalar: 7\n",
      "The tensor dimension scalar.ndim: 0\n",
      "\n",
      "We can get the python number within a tensor with the item() method: 7,\n",
      "altough this only works with one-element tensors\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scalar\n",
    "scalar = torch.tensor(7)\n",
    "print(f\"\"\"\n",
    "The Scalar: {scalar}\n",
    "The tensor dimension scalar.ndim: {scalar.ndim}\n",
    "\n",
    "We can get the python number within a tensor with the item() method: {scalar.item()},\n",
    "altough this only works with one-element tensors\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Vector: tensor([7, 7])\n",
      "The vector dimension: 1\n",
      "The shape tells you have the elements inside the tensors are arranged.\n",
      "For tensor([7, 7]), the shape is torch.Size([2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vectors\n",
    "vector = torch.tensor([7, 7])\n",
    "print(f\"\"\"\n",
    "The Vector: {vector}\n",
    "The vector dimension: {vector.ndim}\n",
    "The shape tells you have the elements inside the tensors are arranged.\n",
    "For {vector}, the shape is {vector.shape}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Matrix: \n",
      "tensor([[ 7,  8],\n",
      "        [ 9, 10]])\n",
      "\n",
      "You notice we have two `[`, so the dimension should be 2.\n",
      "MATRIX.ndim = 2,\n",
      "We have 2 rows and 2 columns, so the shape is MATRIX.shape = torch.Size([2, 2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Matrix\n",
    "MATRIX = torch.tensor(\n",
    "    [[7, 8],\n",
    "     [9, 10]]\n",
    ")\n",
    "print(f\"\"\"\n",
    "The Matrix: \n",
    "{MATRIX}\n",
    "\n",
    "You notice we have two `[`, so the dimension should be 2.\n",
    "MATRIX.ndim = {MATRIX.ndim},\n",
    "We have 2 rows and 2 columns, so the shape is MATRIX.shape = {MATRIX.shape}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Tensor: tensor([[[1, 2, 3],\n",
      "         [3, 6, 9],\n",
      "         [2, 4, 5]]])\n",
      "The tensor dimension: 3\n",
      "The shape of the tensor is torch.Size([1, 3, 3])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tensor\n",
    "TENSOR = torch.tensor(\n",
    "    [[[1,2,3],\n",
    "      [3,6,9],\n",
    "      [2,4,5]]]\n",
    ")\n",
    "print(f\"\"\"\n",
    "The Tensor: {TENSOR}\n",
    "The tensor dimension: {TENSOR.ndim}\n",
    "The shape of the tensor is {TENSOR.shape}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19, 22],\n",
      "        [43, 50]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "A = torch.tensor([[1, 2], [3, 4]])\n",
    "B = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "result = torch.matmul(A, B)  # Tambi√©n puedes usar A @ B\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality\n",
    "Lets create matrixes that are compatible for the multiplication operation. Remember that given a Matrix of shape: $A:^{nxm}$ and $B:^{pxq}$ Then the number of columns in the first matrix should be the same as the number of rows for the second matrix. In other words, the internal dimensions should match.\n",
    "For example:\n",
    "\n",
    "$$\n",
    "A:^{ (2x3) }\n",
    "B:^{ (3x4) }\n",
    "$$\n",
    "\n",
    "Multiplying these matrices will generate a matrix of shape $C:^{2*4}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C=\n",
      "tensor([[0.6103, 0.2330, 0.7837, 0.3185],\n",
      "        [0.9458, 0.6058, 0.7217, 0.7832]])      \n",
      "C has a shape of torch.Size([2, 4])\n",
      "C has a dimension of 2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A  = torch.rand(2,3)\n",
    "B = torch.rand(3,4)\n",
    "\n",
    "C = torch.matmul(A, B)\n",
    "print(f\"\"\"\n",
    "C=\n",
    "{C}      \n",
    "C has a shape of {C.shape}\n",
    "C has a dimension of {C.ndim}\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "# 3D Tensors and Batch Dimensions\n",
    "# When dealing with tensors with more than 2 dimensions (like 3D), PyTorch treats the first dimension as the batch dimension, and it applies the matrix multiplication rule to each pair of matrices in the batch.\n",
    "\n",
    "# For example, the next matrix can be thought of as 2 matrices of size  (3, 4)  stacked along the first dimension.\n",
    "A = torch.rand(2, 3, 4)\n",
    "# And the next can be thought of as 2 matrices of size  (4, 5)  stacked along the first dimension.\n",
    "B = torch.rand(2, 4, 5)\n",
    "# PyTorch performs the matrix multiplication independently for each pair of matrices in the batch (i.e., for each slice along the first dimension).\n",
    "result = torch.matmul(A, B)\n",
    "# Since there are 2 matrices in the batch (from the first dimension), the final result has shape  (2, 3, 5) .\n",
    "print(result.shape)\n",
    "# The resulting tensor shape will be (batch_size, m, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Broadcasting\n",
    "```\n",
    "A = torch.rand(1, 3, 4, 5)\n",
    "B = torch.rand(2, 1, 5, 6)\n",
    "```\n",
    "\n",
    "Imagine we want to multiply A and B. For tensors with dimensions greater than 2D, python treats the outermost left dimension as the \"batch dimensions\",\n",
    "and the outermost 2 right dimensions are multiplied using the matrix multiplication rules.\n",
    "In this case, the matrix multiplication will be performed on the shapes (3, 4) from A and (4, 5) from B,  the result will be a tensor of shape (3, 5).\n",
    "But the Batch dimensions are not compatible due its shapes (1,3) from A and (2,1).\n",
    "\n",
    "In this case pytorch will apply a \"Broadcast\" to these matrices. It consist in \"expanding\" or \"stretching\" the dimensions. This requires that one of the dimensions is 1 which is the case in our exercise.\n",
    "\n",
    "Our matrix A will expand its first dimension twice so ends up with (2,3). And our matrix B will expand is second dimension 3 times, so it will end up in (2,3).\n",
    "Now with a compatible shape, our matrix can be multiplied. Pytorch take care of this process in the matmul operation\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of A: torch.Size([1, 3, 4, 5])\n",
      "Shape of B: torch.Size([2, 1, 5, 6])\n",
      "Shape of result: torch.Size([2, 3, 4, 6])\n"
     ]
    }
   ],
   "source": [
    "A = torch.rand(1, 3, 4, 5)\n",
    "B = torch.rand(2, 1, 5, 6)\n",
    "\n",
    "# Perform matrix multiplication\n",
    "result = torch.matmul(A, B)\n",
    "\n",
    "print(\"Shape of A:\", A.shape)\n",
    "print(\"Shape of B:\", B.shape)\n",
    "print(\"Shape of result:\", result.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand this expansions of dimension, upgrade the dimension on the first dimension on the matrix A\n",
    "``` \n",
    "A = torch.rand(2, 3, 4, 5)\n",
    "B = torch.rand(2, 1, 5, 6)\n",
    "```\n",
    "\n",
    "This will still work because we still have a dimension equals to 1 in the second matrix B. The process of expansion is made in this one, so the first 2 dimensions of B become: (2,3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of A: torch.Size([2, 3, 4, 5])\n",
      "Shape of B: torch.Size([2, 1, 5, 6])\n",
      "Shape of result: torch.Size([2, 3, 4, 6])\n"
     ]
    }
   ],
   "source": [
    "A = torch.rand(2, 3, 4, 5)\n",
    "B = torch.rand(2, 1, 5, 6)\n",
    "\n",
    "# Perform matrix multiplication\n",
    "result = torch.matmul(A, B)\n",
    "\n",
    "print(\"Shape of A:\", A.shape)\n",
    "print(\"Shape of B:\", B.shape)\n",
    "print(\"Shape of result:\", result.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we change to something like:\n",
    "```  \n",
    "A = torch.rand(2, 3, 4, 5)\n",
    "B = torch.rand(2, 2, 5, 6)\n",
    "\n",
    "```\n",
    "\n",
    "We cannot perform the expansion of the matrix and the operation will fail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m B \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Perform matrix multiplication\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of A:\u001b[39m\u001b[38;5;124m\"\u001b[39m, A\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of B:\u001b[39m\u001b[38;5;124m\"\u001b[39m, B\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "A = torch.rand(2, 3, 4, 5)\n",
    "B = torch.rand(2, 2, 5, 6)\n",
    "\n",
    "# Perform matrix multiplication\n",
    "result = torch.matmul(A, B)\n",
    "\n",
    "print(\"Shape of A:\", A.shape)\n",
    "print(\"Shape of B:\", B.shape)\n",
    "print(\"Shape of result:\", result.shape)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common errors in deep learning (shape erros)\n",
    "\n",
    "Now that we have an understanding of basic matrix shape requirements for multiplication lets check the following error examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of A: torch.Size([3, 2])\n",
      "shape of B: torch.Size([3, 2])\n",
      "We notice that the internal dimensions of these tensors are 2 and 3, which means that we CAN'T perform matrix multiplication between these two tensors.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape of B: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor_B\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe notice that the internal dimensions of these tensors are 2 and 3, which means that we CAN\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT perform matrix multiplication between these two tensors.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_A\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_B\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (this will error)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)"
     ]
    }
   ],
   "source": [
    "# Shapes need to be in the right way  \n",
    "tensor_A = torch.tensor([[1, 2],\n",
    "                         [3, 4],\n",
    "                         [5, 6]], dtype=torch.float32)\n",
    "\n",
    "tensor_B = torch.tensor([[7, 10],\n",
    "                         [8, 11], \n",
    "                         [9, 12]], dtype=torch.float32)\n",
    "\n",
    "print(f\"shape of A: {tensor_A.shape}\")\n",
    "print(f\"shape of B: {tensor_B.shape}\")\n",
    "print(f\"We notice that the internal dimensions of these tensors are 2 and 3, which means that we CAN'T perform matrix multiplication between these two tensors.\")\n",
    "torch.matmul(tensor_A, tensor_B) # (this will error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an opportunity to test some techniques for tensor shape manipulation, such as `transpose`.\n",
    "Here we introduce the function: `torch.transpose(input, dim0, dim1)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "Shape: torch.Size([2, 3])\n",
      "\n",
      "Transposed Tensor:\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "Shape: torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "# Create a 2D tensor\n",
    "tensor = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6]])\n",
    "# Original shape\n",
    "print(\"Original Tensor:\")\n",
    "print(tensor)\n",
    "print(f\"Shape: {tensor.shape}\")\n",
    "\n",
    "# Transpose the dimensions (swap rows and columns)\n",
    "transposed_tensor = torch.transpose(\n",
    "    tensor,  # The input tensor has shape  (2, 3) , with 2 rows and 3 columns\n",
    "    0,       # Swaps dimension 0 (rows)\n",
    "    1        # with dimension 1 (columns)\n",
    ")\n",
    "\n",
    "# This will result in a tensor of shape (3,2)\n",
    "\n",
    "print(\"\\nTransposed Tensor:\")\n",
    "print(transposed_tensor)\n",
    "print(f\"Shape: {transposed_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]])\n",
      "shape of tensor_A: torch.Size([3, 2])\n",
      "tensor([[ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n",
      "shape of tensor_B.T: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# We can also to this using T, lets try on our prevous tensors: tensor_A and tensor_B\n",
    "# View tensor_A and tensor_B.T\n",
    "print(tensor_A)\n",
    "print(f\"shape of tensor_A: {tensor_A.shape}\")\n",
    "print(tensor_B.T)\n",
    "print(f\"shape of tensor_B.T: {tensor_B.T.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 27.,  30.,  33.],\n",
       "        [ 61.,  68.,  75.],\n",
       "        [ 95., 106., 117.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We now get the same internal dimensions, so we can perform matrix multiplication\n",
    "torch.matmul(tensor_A, tensor_B.T)\n",
    "# Oh btw, we can use the shortcut torch.mm(tensor_A, tensor_B.T) to perform matrix multiplication\n",
    "torch.mm(tensor_A, tensor_B.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A practical example of matrix multiplication\n",
    "\n",
    "lets review the `torch.nn.Linear()` module, also know as a feed forward layer or fully connected layer. It implmements a matrix multiplication between an input `x`and a weights matrix `A`.\n",
    "\n",
    "You may recognize this in the function use to compute the output of a neural net:\n",
    "\n",
    "$$ y = x\\cdot{W^T} + b $$\n",
    "\n",
    "Where:\n",
    "\n",
    "- `x` is the input to the layer (deep learning is a stack of layers like torch.nn.Linear() and others on top of each other).\n",
    "- `W` is the weights matrix created by the layer, this starts out as random numbers that get adjusted as a neural network learns to better represent patterns in the data (notice the \"`T`\",  - that's because the weights matrix gets transposed).\n",
    "\n",
    "- `b` is the bias term used to slightly offset the weights and inputs.\n",
    "- `y` is the output (a manipulation of the input in the hopes to discover patterns in it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]])\n",
      "x_shape: torch.Size([3, 2]) this means that we have 3 samples with 2 features each\n",
      "\n",
      "\n",
      "Weight matrix W:\n",
      "tensor([[ 0.5406,  0.5869],\n",
      "        [-0.1657,  0.6496],\n",
      "        [-0.1549,  0.1427],\n",
      "        [-0.3443,  0.4153],\n",
      "        [ 0.6233, -0.5188],\n",
      "        [ 0.6146,  0.1323]])\n",
      "\n",
      "Weight matrix shape: torch.Size([6, 2])\n",
      "When transposed the weight matrix will be:\n",
      "tensor([[ 0.5406, -0.1657, -0.1549, -0.3443,  0.6233,  0.6146],\n",
      "        [ 0.5869,  0.6496,  0.1427,  0.4153, -0.5188,  0.1323]]) \n",
      " with shape: torch.Size([2, 6])\n",
      "Bias vector b:\n",
      "tensor([ 0.5224,  0.0958,  0.3410, -0.0998,  0.5451,  0.1045])\n",
      "\n",
      "Bias vector shape: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# Since the linear layer starts with a random weights matrix, let's make it reproducible (more on this later)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Let's begin by defining our input tensor `x``\n",
    "# Our input tensor has a shape of (3, 2). \n",
    "x = tensor_A\n",
    "print(f\"x: {x}\\nx_shape: {x.shape} this means that we have 3 samples with 2 features each\\n\\n\")\n",
    "\n",
    "# The `linear` function will declare:\n",
    "\n",
    "# The weight matrix `W` as (out_features=6, in_features=2)\n",
    "#   Let's paue to analyze this initialization shape: (6,2)\n",
    "#   The first dimension (6) represent the rows of the weight matrix W, and they represent the neurons (or output features) of the layer\n",
    "#   The second dimension (2) represent the columns of the weight matrix W, and they represent the input features. Thus in inner dimension of the input should match this value\n",
    "\n",
    "\n",
    "# A bias vector b of shpe (out_features=6) is also initialized\n",
    "linear = torch.nn.Linear(\n",
    "                out_features=6, # out_features = describes outer value \n",
    "                in_features=2, # in_features = matches inner dimension of input                 \n",
    "                bias=True\n",
    ") \n",
    "\n",
    "# Weight matrix\n",
    "W = linear.weight.data\n",
    "print(f\"Weight matrix W:\\n{W}\\n\\nWeight matrix shape: {W.shape}\")\n",
    "print(f\"When transposed the weight matrix will be:\\n{W.T} \\n with shape: {W.T.shape}\")\n",
    "\n",
    "# Get bias vector \n",
    "bias = linear.bias.data \n",
    "print(f\"Bias vector b:\\n{bias}\\n\\nBias vector shape: {bias.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "tensor([[2.2368, 1.2292, 0.4714, 0.3864, 0.1309, 0.9838],\n",
      "        [4.4919, 2.1970, 0.4469, 0.5285, 0.3401, 2.4777],\n",
      "        [6.7469, 3.1648, 0.4224, 0.6705, 0.5493, 3.9716]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Output shape: torch.Size([3, 6])\n",
      "Output shape: torch.Size([3, 6])\n",
      "First dimension of shape:\n",
      "3 = Number of samples in the batch.\n",
      "Each row is computed independently by applying the same weights matrix W and bias to each input sample\n",
      "Second dimension of shape:\n",
      "6 = Number of output features/neurons in the layer\n",
      "Each column corresponds to the activation of a specific neuron for a given input sample\n"
     ]
    }
   ],
   "source": [
    "# Now we will perform the operation x¬∑W_T + b \n",
    "# (3,2)¬∑(6,2)^T + (b) = (3,2)¬∑(2,6)= (3,6) \n",
    "output = linear(x)\n",
    "print(f\"Output:\\n{output}\\n\\nOutput shape: {output.shape}\")\n",
    "\n",
    "\n",
    "# We can interpret the output as follows:\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"First dimension of shape:\\n{output.shape[0]} = Number of samples in the batch.\\nEach row is computed independently by applying the same weights matrix W and bias to each input sample\")\n",
    "print(f\"Second dimension of shape:\\n{output.shape[1]} = Number of output features/neurons in the layer\\nEach column corresponds to the activation of a specific neuron for a given input sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshaping, stacking, squeezing and unsqueezing\n",
    "\n",
    "We will review some common functions for reshaping\n",
    "\n",
    "## Reshape: `torch.reshape(input, shape)`\n",
    "Preserves Total Number of Elements:\n",
    "- The new shape must maintain the same total number of elements as the original tensor.\n",
    "- For example:\n",
    "    - A tensor of shape  `(2, 3)`  has  $2 \\times 3 = 6$  elements.\n",
    "    - You can reshape it to  (1, 6) ,  (6, 1) , or any shape that results in  6  elements.\n",
    "\n",
    "\n",
    "Creates a New View:\n",
    "- The reshaped tensor shares the same underlying data as the original tensor, meaning no new memory is allocated.\n",
    "- Any changes to the reshaped tensor will reflect in the original tensor, and vice versa, as long as the tensor is contiguous.\n",
    "\n",
    "A simple way of understanding this is that, as long as the final shape (2,3) equals the number of elmements (6), it can take any form such as (3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor: tensor([1., 2., 3., 4., 5., 6.])\n",
      "Original Shape: torch.Size([6])\n",
      "\n",
      "Reshaped (2x3):\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "\n",
      "Reshaped (3x2):\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "# Lets create a tensor with 6 elements\n",
    "x = torch.arange(1., 7.)  # Shape: (6,)\n",
    "print(\"Original Tensor:\", x)\n",
    "print(\"Original Shape:\", x.shape)\n",
    "\n",
    "# we can reshape it to a 2x3 tensor\n",
    "reshaped1 = x.reshape(2, 3)\n",
    "# or a 3x2 tensor\n",
    "reshaped2 = x.reshape(3, 2)\n",
    "\n",
    "print(f\"\\nReshaped (2x3):\\n{reshaped1}\")\n",
    "print(f\"\\nReshaped (3x2):\\n{reshaped2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: \n",
      "tensor([1., 2., 3., 4., 5., 6., 7.])\n",
      "Shape: torch.Size([7])\n",
      "size: torch.Size([7])\n",
      "x_reshaped: \n",
      "tensor([[1., 2., 3., 4., 5., 6., 7.]])\n",
      "Shape: torch.Size([1, 7])\n",
      "size: torch.Size([1, 7])\n"
     ]
    }
   ],
   "source": [
    "# IN this example we will add an extra dimension\n",
    "\n",
    "x = torch.arange(1., 8.)\n",
    "print(f\"x: \\n{x}\\nShape: {x.shape}\\nsize: {x.size()}\")\n",
    "\n",
    "# Add an extra dimension\n",
    "x_reshaped = x.reshape(1, 7)\n",
    "print(f\"x_reshaped: \\n{x_reshaped}\\nShape: {x_reshaped.shape}\\nsize: {x_reshaped.size()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: \n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "Shape: torch.Size([12])\n",
      "reshaped:\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "Shape: torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "# you can specify one dimension as -1, and PyTorch will automatically infer its size to ensure the total number of elements remains constan\n",
    "\n",
    "x = torch.arange(12)  # Shape: (12,)\n",
    "reshaped = x.reshape(3, -1)  # Shape: (3, 4)\n",
    "print(f\"x: \\n{x}\\nShape: {x.shape}\")\n",
    "print(f\"reshaped:\\n{reshaped}\\nShape: {reshaped.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Tensor.view(shape)`\n",
    "Returns a view of the original tensor in a different shape but shares the same data as the original tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: \n",
      "tensor([1., 2., 3., 4., 5., 6., 7.])\n",
      "Shape: torch.Size([7])\n",
      "z: \n",
      "tensor([[1., 2., 3., 4., 5., 6., 7.]])\n",
      "Shape: torch.Size([1, 7])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1., 8.)  # Shape: (12,)\n",
    "\n",
    "print(f\"x: \\n{x}\\nShape: {x.shape}\")\n",
    "z = x.view(1,7)\n",
    "print(f\"z: \\n{z}\\nShape: {z.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5., 2., 3., 4., 5., 6., 7.]]), tensor([5., 2., 3., 4., 5., 6., 7.]))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remember though, changing the view of a tensor with torch.view() really only creates a new view of the same tensor.\n",
    "# So changing the view changes the original tensor too\n",
    "z[:, 0] = 5 \n",
    "z, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack torch.stack(tensors, dim=0)\n",
    "Concatenates a sequence of tensors along a new dimension (dim), all tensors must be same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original x: \n",
      "tensor([5., 2., 3., 4., 5., 6., 7.])\n",
      "Shape: torch.Size([7])\n",
      "\n",
      "x_stacked: \n",
      "tensor([[5., 2., 3., 4., 5., 6., 7.],\n",
      "        [5., 2., 3., 4., 5., 6., 7.],\n",
      "        [5., 2., 3., 4., 5., 6., 7.]])\n",
      "Shape: torch.Size([3, 7])\n"
     ]
    }
   ],
   "source": [
    "# Stacking in dimension 0 will place the stacked tensors as rows\n",
    "x_stacked_dim0 = torch.stack([x, x, x], dim=0)\n",
    "print(f\"original x: \\n{x}\\nShape: {x.shape}\\n\")\n",
    "print(f\"x_stacked: \\n{x_stacked_dim0}\\nShape: {x_stacked_dim0.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original x: \n",
      "tensor([5., 2., 3., 4., 5., 6., 7.])\n",
      "Shape: torch.Size([7])\n",
      "\n",
      "x_stacked: \n",
      "tensor([[5., 5., 5.],\n",
      "        [2., 2., 2.],\n",
      "        [3., 3., 3.],\n",
      "        [4., 4., 4.],\n",
      "        [5., 5., 5.],\n",
      "        [6., 6., 6.],\n",
      "        [7., 7., 7.]])\n",
      "Shape: torch.Size([7, 3])\n"
     ]
    }
   ],
   "source": [
    "# Stacking in dimension 1 will place the stacked tensors as columns\n",
    "x_stacked_dim1 = torch.stack([x, x, x], dim=1)\n",
    "print(f\"original x: \\n{x}\\nShape: {x.shape}\\n\")\n",
    "print(f\"x_stacked: \\n{x_stacked_dim1}\\nShape: {x_stacked_dim1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.squeeze(input)\n",
    "Squeezes input to remove all the dimenions with value 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: \n",
      "tensor([[0.6135, 0.0086, 0.7622, 0.6847, 0.5212, 0.7146, 0.5006]])\n",
      "Shape: torch.Size([1, 7])\n",
      "x_squeezed: \n",
      "tensor([0.6135, 0.0086, 0.7622, 0.6847, 0.5212, 0.7146, 0.5006])\n",
      "Shape: torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 7)\n",
    "print(f\"x: \\n{x}\\nShape: {x.shape}\")\n",
    "\n",
    "x_squeezed = torch.squeeze(x)\n",
    "print(f\"x_squeezed: \\n{x_squeezed}\\nShape: {x_squeezed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.unsqueeze(input, dim)\n",
    "\n",
    "Returns input with a dimension value of 1 added at dim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_unsqueezed: \n",
      "tensor([[0.6135, 0.0086, 0.7622, 0.6847, 0.5212, 0.7146, 0.5006]])\n",
      "Shape: torch.Size([1, 7])\n"
     ]
    }
   ],
   "source": [
    "x_unsqueezed = x_squeezed.unsqueeze(dim=0)\n",
    "print(f\"x_unsqueezed: \\n{x_unsqueezed}\\nShape: {x_unsqueezed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.permute(input, dims)\n",
    "\n",
    "Returns a view of the original input with its dimensions permuted (rearranged) to dims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous shape: torch.Size([224, 225, 3])\n",
      "New shape: torch.Size([3, 224, 225])\n"
     ]
    }
   ],
   "source": [
    "# Create tensor with specific shape\n",
    "x_original = torch.rand(size=(224, 225, 3))\n",
    "\n",
    "# Permute the original tensor to rearrange the axis order\n",
    "x_permuted = x_original.permute(2, 0, 1) # shifts axis 0->1, 1->2, 2->0\n",
    "\n",
    "print(f\"Previous shape: {x_original.shape}\")\n",
    "print(f\"New shape: {x_permuted.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting data from tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1, 2, 3],\n",
       "          [4, 5, 6],\n",
       "          [7, 8, 9]]]),\n",
       " torch.Size([1, 3, 3]))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(1, 10).reshape(1, 3, 3)\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First square bracket:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "Second square bracket: tensor([1, 2, 3])\n",
      "Third square bracket: 1\n"
     ]
    }
   ],
   "source": [
    "# Let's index bracket by bracket\n",
    "print(f\"First square bracket:\\n{x[0]}\") \n",
    "print(f\"Second square bracket: {x[0][0]}\") \n",
    "print(f\"Third square bracket: {x[0][0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all values of 0th dimension and the 0 index of 1st dimension\n",
    "x[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 5, 8]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all values of 0th & 1st dimensions but only index 1 of 2nd dimension\n",
    "x[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all values of the 0 dimension but only the 1 index value of the 1st and 2nd dimension\n",
    "x[:, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get index 0 of 0th and 1st dimension and all values of 2nd dimension \n",
    "x[0, 0, :] # same as x[0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aurora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
